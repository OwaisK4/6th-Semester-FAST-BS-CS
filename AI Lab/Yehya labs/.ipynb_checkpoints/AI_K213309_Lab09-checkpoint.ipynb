{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0gxzwfmL2TL"
   },
   "source": [
    "#AI Lab 09\n",
    "K213309 | BCS-6F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkgX0iKTXYSQ"
   },
   "source": [
    "##Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8SRq5_2Lw6Z",
    "outputId": "b84b1096-5d41-424e-8cb7-dc6e6716825d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "069xVKCrXPRN"
   },
   "source": [
    "##Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GFwVN85XRWv",
    "outputId": "0ef4aaaf-ee8b-4555-d046-3220a289a01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "'''Implement Q-learning algorithm using OpenAI gym environment.'''\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "total_episodes = 20000\n",
    "learning_rate = 0.7\n",
    "max_steps = 99\n",
    "gamma = 0.7\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.5\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoGUoqB6L65N"
   },
   "source": [
    "##Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vof9e9YXMZls",
    "outputId": "43cf3080-9c74-4255-dc78-fad703cb44bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "*****************************************\n",
      "Training is done!\n",
      "\n",
      "*****************************************\n"
     ]
    }
   ],
   "source": [
    "'''The Smart cab's job is to pick up the passenger at one location and drop them off in another.\n",
    "The agent should receive a high positive reward for a successful drop off because this\n",
    "behavior is highly desired\n",
    "The agent should be penalized if it tries to drop off a passenger in wrong locations\n",
    "The agent should get a slight negative reward for not making it to the destination after\n",
    "every time-step.\n",
    "The passenger can be in one of the four possible locations: R, G, Y, B, which are\n",
    "represented in row, column coordinates as (0,0), (0,4), (4,0), (4,3) respectively.\n",
    "Additionally, we need to consider a fifth state where the passenger is already inside the\n",
    "taxi. Therefore, the number of possible states for the passenger's location is 5.\n",
    "The destination can be one of the four possible locations: R, G, Y, B, which are also\n",
    "represented in row, column coordinates. Therefore, the number of possible states for the\n",
    "destination is 4.\n",
    "We have six possible actions:\n",
    "1. south\n",
    "2. north\n",
    "3. east\n",
    "4. west\n",
    "5. pickup\n",
    "6. drop off\n",
    "Implement the above problem using Gym environment called Taxi-V2.'''\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print('Number of states:{}'.format(env.observation_space.n))\n",
    "print('Number of actions:{}'.format(env.action_space.n))\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "num_of_episodes = 100000\n",
    "for episode in range(0 ,num_of_episodes):\n",
    "    # reset the environment\n",
    "    state = env.reset()\n",
    "\n",
    "    # Initialized the variables\n",
    "    reward = 0\n",
    "    terminated = False\n",
    "    # if we want to not terminale the loop\n",
    "    while not terminated:\n",
    "        # Take learned path or explore new actions based on the epsilon\n",
    "        if random.uniform(0,1)<epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "            # Take action\n",
    "            next_state, reward, terminated, info = env.step(action)\n",
    "\n",
    "\n",
    "            # Recalculate\n",
    "            q_value = q_table[state , action]\n",
    "            max_value = np.max(q_table[next_state])\n",
    "            new_q_value = (1-alpha)* q_value + alpha * (reward + gamma * max_value)\n",
    "\n",
    "\n",
    "            # update q_table\n",
    "            q_table[state , action] = new_q_value\n",
    "            state = next_state\n",
    "    if (episode + 1)%100 == 0:\n",
    "                clear_output(wait = True)\n",
    "                print('Episode: {}'.format(episode + 1))\n",
    "                env.render()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('*****************************************')\n",
    "print('Training is done!\\n')\n",
    "print('*****************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc-kUTozXSHn"
   },
   "source": [
    "##Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu0WRooAXT1O"
   },
   "outputs": [],
   "source": [
    "'''A well equipped state of the art hospital monitors data of the patients using AI devices\n",
    "remotely. Each patient has different sensors deployed on his/her body to acquire the values\n",
    "of sugar, Blood Pressure etc. Day and night expelling of the data requires infinite lifetime\n",
    "which is the ideal situation and nearly impossible due to DC devices deployment but to\n",
    "increase its lifetime we can go for certain AI techniques e.g reinforcement learning by\n",
    "measuring its nearest path to the sink from the cluster head. Develop such algorithm to\n",
    "increase the network lifetime upto maximum extent using Reinforcement learning. Your\n",
    "code must contain the cost factor while calculating the reward function for the next state.'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qkgX0iKTXYSQ",
    "069xVKCrXPRN",
    "YoGUoqB6L65N"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
